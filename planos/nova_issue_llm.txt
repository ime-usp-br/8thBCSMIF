TITLE: [DevTools][Feature] Criar script Python para automatizar interação com Gemini via meta-prompts
TYPE: feature
LABELS: devtools,automation,llm,python,feature,todo
ASSIGNEE: @me
PROJECT: laravel_12_starter_kit
FEATURE_MOTIVATION: Agilizar e padronizar o uso de LLMs (Google Gemini) no fluxo de desenvolvimento, aproveitando o contexto do projeto coletado pelo script `gerar_contexto_llm.sh` e reduzindo a necessidade de interação manual via AI Studio. Esta issue é um desdobramento da issue #20.
FEATURE_DESCRIPTION: Desenvolver um script Python que utilize a biblioteca `google.genai` para interagir com a API do Gemini (modelo Pro 2.5 ou similar). O script deve:
1.  Receber como entrada o caminho para um template de meta-prompt e possíveis variáveis (via argumentos de linha de comando).
2.  Identificar o diretório de contexto mais recente gerado por `gerar_contexto_llm.sh`.
3.  Preencher o template de meta-prompt com as variáveis fornecidas.
4.  Iniciar uma primeira interação com a API do Gemini:
    - Anexar os arquivos de contexto relevantes (todos os arquivos .txt/.json do diretório de contexto).
    - Submeter o meta-prompt preenchido.
5.  Processar a resposta do Gemini para extrair o prompt final gerado.
6.  Iniciar  uma nova conversa com a API do Gemini:
    - Anexar novamente os arquivos de contexto relevantes.
    - Submeter o prompt final extraído na etapa anterior.
7.  Capturar a resposta final da IA.
8.  Salvar a resposta final em um arquivo dentro de um diretório específico (ex: `./llm_outputs/<nome_tarefa>/<timestamp>.txt`), garantindo que este diretório esteja no `.gitignore`.
PROPOSED_SOLUTION:
- Utilizar a biblioteca oficial `google.genai` para Python. [Pesquisar documentação](https://ai.google.dev/docs/python_genai_quickstart) para entender como anexar múltiplos arquivos de contexto e gerenciar conversas.
- Definir um formato padrão para os meta-prompts (localizados em `project_templates/meta-prompts/`).
- Gerenciar a API Key do Google AI Studio via variável de ambiente (ex: `GEMINI_API_KEY`).
- Implementar lógica para encontrar o diretório de contexto mais recente em `./code_context_llm/`.
- Definir como as variáveis serão passadas para o script (argumentos CLI, arquivo JSON/YAML?).
- Implementar tratamento básico de erros (falha na API, arquivo não encontrado).
- Estruturar o script Python de forma modular (funções para carregar contexto, preencher template, chamar API, salvar resultado).
ACCEPTANCE_CRITERIA:
- [ ] Script Python (`llm_interact.py` ou similar) existe no repositório (provavelmente na raiz ou em `scripts/`).
- [ ] Script aceita o nome/caminho de um meta-prompt como argumento.
- [ ] Script localiza e utiliza o diretório de contexto mais recente de `code_context_llm/`.
- [ ] Script utiliza a API Key da variável de ambiente `GEMINI_API_KEY`.
- [ ] Script realiza a interação em duas etapas com o Gemini (meta-prompt -> prompt final -> resposta final) anexando os arquivos de contexto em ambas as etapas relevantes.
- [ ] Script salva a resposta final da IA em um arquivo dentro de um diretório não versionado (ex: `llm_outputs/`).
- [ ] Script possui tratamento básico para erros de API e arquivos não encontrados.
- [ ] O diretório de saída (`llm_outputs/`) está adicionado ao `.gitignore`.
- [ ] Código Python segue padrões básicos de estilo (ex: PEP 8).
- [ ] Documentação do projeto reflete as mudanças.
