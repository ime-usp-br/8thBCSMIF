#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
llm_interact.py: Script to interact with the Google Gemini API using project context.

This script automates interactions with the Gemini LLM, using context files
generated by 'gerar_contexto_llm.sh' and meta-prompts to streamline
development tasks like generating commit messages, analyzing code, etc.
"""

import argparse
import os
import sys
from pathlib import Path
import re
import google.genai as genai
from google.genai import types
from google.genai import errors # Import the error module
from dotenv import load_dotenv
import traceback # For debugging unexpected errors
from datetime import datetime # For timestamping output files (AC8)

# --- Configuration ---
# Assumes the script is in /scripts and templates in /project_templates/meta-prompts
BASE_DIR = Path(__file__).resolve().parent.parent
META_PROMPT_DIR = BASE_DIR / "project_templates/meta-prompts"
CONTEXT_DIR_BASE = BASE_DIR / "context_llm/code"
COMMON_CONTEXT_DIR = BASE_DIR / "context_llm/common" # Directory for common context files (AC7)
OUTPUT_DIR_BASE = BASE_DIR / "llm_outputs" # Directory for saving outputs, should be in .gitignore (AC8)
TIMESTAMP_DIR_REGEX = r'^\d{8}_\d{6}$' # Regex to validate directory name format
# Gemini model to use (choose an appropriate model for tasks)
GEMINI_MODEL_GENERAL_TASKS = 'gemini-2.5-pro-exp-03-25' # Do not change
GEMINI_MODEL_RESOLVE = 'gemini-2.5-pro-exp-03-25' # Do not change

def find_available_tasks(prompt_dir: Path) -> dict[str, Path]:
    """
    Find available tasks (meta-prompts) in the specified directory.

    Args:
        prompt_dir: The Path to the directory containing meta-prompt files.

    Returns:
        A dictionary mapping task names to the Paths of the files.
        Returns an empty dictionary if the directory doesn't exist or contains no prompts.
    """
    tasks = {}
    if not prompt_dir.is_dir():
        print(f"Error: Meta-prompt directory not found: {prompt_dir}", file=sys.stderr)
        return tasks
    # Expected pattern: meta-prompt-task_name.txt
    for filepath in prompt_dir.glob("meta-prompt-*.txt"):
        if filepath.is_file():
            task_name = filepath.stem.replace("meta-prompt-", "")
            if task_name:
                tasks[task_name] = filepath
    return tasks

def find_latest_context_dir(context_base_dir: Path) -> Path | None:
    """
    Find the most recent context directory within the base directory.

    Args:
        context_base_dir: The Path to the base directory where context
                          directories (timestamped) are located.

    Returns:
        A Path object for the latest directory found, or None if
        no valid directory is found or the base directory doesn't exist.
    """
    if not context_base_dir.is_dir():
        print(f"Error: Context base directory not found: {context_base_dir}", file=sys.stderr)
        return None

    valid_context_dirs = []
    for item in context_base_dir.iterdir():
        # Ensure it's a directory and matches the timestamp format, excluding the common dir
        if item.is_dir() and re.match(TIMESTAMP_DIR_REGEX, item.name):
            valid_context_dirs.append(item)

    if not valid_context_dirs:
        print(f"Error: No valid timestamped context directories (YYYYMMDD_HHMMSS format) found in {context_base_dir}", file=sys.stderr)
        return None

    # Sort directories by name (timestamp) in descending order
    latest_context_dir = sorted(valid_context_dirs, reverse=True)[0]
    return latest_context_dir

def load_and_fill_template(template_path: Path, variables: dict) -> str:
    """
    Load a meta-prompt template and replace placeholders with provided variables.

    Args:
        template_path: The Path to the template file.
        variables: A dictionary where keys are variable names (without __)
                   and values are the data for substitution.

    Returns:
        The template content with variables substituted.
        Returns an empty string if the template cannot be read or an error occurs.
    """
    try:
        content = template_path.read_text(encoding='utf-8')
        # Helper function to handle substitution
        def replace_match(match):
            var_name = match.group(1)
            # Returns the variable value from the dictionary or an empty string if not found
            # Ensures the value is a string for substitution
            return str(variables.get(var_name, ''))

        # Regex to find placeholders like __VARIABLE_EXAMPLE__
        filled_content = re.sub(r'__([A-Z_]+)__', replace_match, content)
        return filled_content
    except FileNotFoundError:
        print(f"Error: Template file not found: {template_path}", file=sys.stderr)
        return ""
    except Exception as e:
        print(f"Error reading or processing template {template_path}: {e}", file=sys.stderr)
        return ""


def _load_files_from_dir(context_dir: Path, context_parts: list[types.Part]) -> None:
    """Helper function to load .txt, .json and .md files from a directory into context_parts."""
    file_patterns = ["*.txt", "*.json", "*.md"]
    loaded_count = 0
    if not context_dir or not context_dir.is_dir():
        print(f"    - Directory not found or invalid: {context_dir}", file=sys.stderr)
        return

    print(f"    Scanning directory: {context_dir.relative_to(BASE_DIR)}")
    for pattern in file_patterns:
        for filepath in context_dir.glob(pattern):
            if filepath.is_file():
                try:
                    # print(f"      - Reading {filepath.name}") # Verbose logging removed
                    content = filepath.read_text(encoding='utf-8')
                    # Add file name at the beginning of content for LLM origin tracking
                    # Use keyword argument 'text='
                    context_parts.append(types.Part.from_text(text=f"--- START OF FILE {filepath.name} ---\n{content}\n--- END OF FILE {filepath.name} ---"))
                    loaded_count += 1
                except Exception as e:
                    print(f"      - Warning: Could not read file {filepath.name}: {e}", file=sys.stderr)
    if loaded_count == 0:
        print(f"      - No context files (.txt, .json) found in this directory.")

def prepare_context_parts(primary_context_dir: Path, common_context_dir: Path | None = None) -> list[types.Part]:
    """
    List context files (.txt, .json) from primary and optionally common directories,
    and prepare them as types.Part.

    Args:
        primary_context_dir: The Path to the primary (e.g., timestamped) context directory.
        common_context_dir: Optional Path to the common context directory.

    Returns:
        A list of types.Part objects representing the content of the files.
    """
    context_parts = []
    print("  Loading context files...")

    # Load from primary directory
    print("  Loading from primary context directory...")
    _load_files_from_dir(primary_context_dir, context_parts)

    # Load from common directory (AC7)
    if common_context_dir:
        print("\n  Loading from common context directory...")
        if common_context_dir.exists() and common_context_dir.is_dir():
             _load_files_from_dir(common_context_dir, context_parts)
        else:
             print(f"    - Common context directory not found or is not a directory: {common_context_dir}")

    print(f"\n  Total context files loaded: {len(context_parts)}.")
    return context_parts

def save_llm_response(task_name: str, response_content: str) -> None:
    """
    Saves the LLM's final response to a timestamped file within a task-specific directory.

    Args:
        task_name: The name of the task (e.g., 'resolve-ac', 'commit-mesage').
        response_content: The string content of the LLM's final response.
    """
    try:
        task_output_dir = OUTPUT_DIR_BASE / task_name
        task_output_dir.mkdir(parents=True, exist_ok=True) # Create dirs if they don't exist

        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"{timestamp_str}.txt" # Or use a more specific extension if needed, e.g., .diff, .md
        output_filepath = task_output_dir / output_filename

        output_filepath.write_text(response_content, encoding='utf-8')
        print(f"  LLM Response saved to: {output_filepath.relative_to(BASE_DIR)}")

    except OSError as e:
        print(f"Error creating output directory {task_output_dir}: {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error saving LLM response to file: {e}", file=sys.stderr)
        traceback.print_exc()

def parse_arguments(available_tasks: list[str]) -> argparse.Namespace:
    """
    Parse command-line arguments, including examples in the epilog.

    Args:
        available_tasks: A list of available task names.

    Returns:
        A Namespace object containing the parsed arguments.
    """
    script_name = Path(sys.argv[0]).name # Get script name for examples

    # --- Build epilog string with examples ---
    epilog_lines = ["\nExamples:"]
    sorted_tasks = sorted(available_tasks)

    for task_name in sorted_tasks:
        if task_name == "commit-mesage":
            example = f"  {script_name} {task_name} --issue 28"
            epilog_lines.append(example)
        elif task_name == "resolve-ac":
            # <<< MODIFICADO AQUI: Usa --observation >>>
            example = f"  {script_name} {task_name} --issue 28 --ac 5 --observation \"Certifique-se de que a API key é lida *apenas* do .env e não das variáveis de sistema.\""
            epilog_lines.append(example)
        elif task_name == "analise-ac":
            example = f"  {script_name} {task_name} --issue 28 --ac 4"
            epilog_lines.append(example)
        else:
            # Generic example for other tasks
            example = f"  {script_name} {task_name} [--issue ISSUE] [--ac AC] [--observation OBSERVATION]"
            epilog_lines.append(example)

    epilog_text = "\n".join(epilog_lines)

    # --- Create parser ---
    parser = argparse.ArgumentParser(
        description="Interact with Google Gemini using project context and meta-prompts.",
        epilog=epilog_text, # Add examples to the help message end
        formatter_class=argparse.RawDescriptionHelpFormatter # Preserve formatting
    )

    task_choices_str = ", ".join(sorted_tasks)
    parser.add_argument(
        "task",
        choices=sorted_tasks,
        help=(f"The task to perform, based on available meta-prompts in "
              f"'{META_PROMPT_DIR.relative_to(BASE_DIR)}'.\nAvailable tasks: {task_choices_str}"),
        metavar="TASK"
    )
    # --- Arguments for meta-prompt variables ---
    parser.add_argument("--issue", help="Issue number (e.g., 28). Fills __NUMERO_DA_ISSUE__.")
    parser.add_argument("--ac", help="Acceptance Criteria number (e.g., 3). Fills __NUMERO_DO_AC__.")
    parser.add_argument("--observation", help="Additional observation/instruction for the task. Fills __OBSERVACAO_ADICIONAL__.", default="")


    return parser.parse_args()

# --- Main Execution ---
if __name__ == "__main__":
    # --- Load .env variables ---
    dotenv_path = BASE_DIR / '.env'
    if dotenv_path.is_file():
        print(f"Loading environment variables from: {dotenv_path.relative_to(BASE_DIR)}")
        load_dotenv(dotenv_path=dotenv_path, verbose=True)
    else:
        print(f"Warning: .env file not found at {dotenv_path}. Relying on system environment variables.", file=sys.stderr)
    # --- End Load .env ---

    # --- Configure GenAI Client with API Key ---
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        print("Error: GEMINI_API_KEY environment variable not set (checked both system env and .env file).", file=sys.stderr)
        print("Please set the GEMINI_API_KEY in your .env file or as a system environment variable.", file=sys.stderr)
        sys.exit(1)
    try:
        # Note: Not specifying 'vertexai=True', uses Gemini Developer API by default.
        genai_client = genai.Client(api_key=api_key)
        print("Google GenAI Client initialized successfully.")
    except Exception as e:
        print(f"Error initializing Google GenAI Client: {e}", file=sys.stderr)
        sys.exit(1)
    # --- End Configure GenAI Client ---

    available_tasks_dict = find_available_tasks(META_PROMPT_DIR)
    available_task_names = list(available_tasks_dict.keys())

    if not available_task_names:
        print(f"Error: No meta-prompt files found in '{META_PROMPT_DIR}'. Exiting.", file=sys.stderr)
        sys.exit(1)

    try:
        # parse_arguments now uses the task list for better help messages
        args = parse_arguments(available_task_names)
        selected_task = args.task
        selected_meta_prompt_path = available_tasks_dict[selected_task]
        GEMINI_MODEL = GEMINI_MODEL_RESOLVE if selected_task == "resolve-ac" else GEMINI_MODEL_GENERAL_TASKS

        print(f"\nLLM Interaction Script") # Add newline for separation from dotenv output
        print(f"========================")
        print(f"Selected Task: {selected_task}")
        print(f"Using Meta-Prompt: {selected_meta_prompt_path.relative_to(BASE_DIR)}")

        # --- Find latest context directory ---
        latest_context_dir = find_latest_context_dir(CONTEXT_DIR_BASE)
        if latest_context_dir is None:
            print("Fatal Error: Could not find a valid context directory. Exiting.", file=sys.stderr)
            sys.exit(1)
        print(f"Latest Context Directory: {latest_context_dir.relative_to(BASE_DIR)}")
        # --- End Find latest context ---

        # --- Collect variables and fill meta-prompt ---
        task_variables = {
            "NUMERO_DA_ISSUE": args.issue if args.issue else "",
            "NUMERO_DO_AC": args.ac if args.ac else "",
            "OBSERVACAO_ADICIONAL": args.observation
        }
        print(f"Variables for template: {task_variables}")

        meta_prompt_content = load_and_fill_template(selected_meta_prompt_path, task_variables)

        if not meta_prompt_content:
             print(f"Error loading or filling the meta-prompt. Exiting.", file=sys.stderr)
             sys.exit(1)

        print(f"------------------------")
        print(f"Filled Meta-Prompt:")
        print(meta_prompt_content) # Print the full filled meta-prompt for clarity
        print(f"------------------------")
        # --- End Collect variables ---

        # --- Two-Step Interaction with Gemini API ---
        print("\nStarting interaction with Gemini API...")

        # Prepare context parts (reading .txt and .json files from latest and common dirs)
        # AC7: Pass both latest and common context directory paths
        context_parts = prepare_context_parts(latest_context_dir, COMMON_CONTEXT_DIR)
        if not context_parts:
             print("Warning: No context files loaded. The AI might lack sufficient information.", file=sys.stderr)

        # --- Step 1: Meta-Prompt + Context -> Final Prompt ---
        print(f"\nStep 1: Sending Meta-Prompt and Context to get the Final Prompt (Model: {GEMINI_MODEL})...")
        prompt_final_content = None
        # Pass prompt using keyword argument 'text='
        contents_etapa1 = [types.Part.from_text(text=meta_prompt_content)] + context_parts
        try:
            # Documentation (google-genai.md) suggests a list of Parts
            # is converted to a single UserContent.
            response_etapa1 = genai_client.models.generate_content(
                model=GEMINI_MODEL,
                contents=contents_etapa1 # Pass the list of parts directly
            )
            # Extract the final prompt from the response
            prompt_final_content = response_etapa1.text
            print("  Final Prompt received:")
            print("  ```")
            print(prompt_final_content.strip())
            print("  ```")
        except errors.APIError as e:
            print(f"  Gemini API Error (Step 1): Code {e.code} - {e.message}", file=sys.stderr)
            print(f"  Error details: {e}") # Print more details if available
            sys.exit(1)
        except Exception as e:
            print(f"  Unexpected Error in Step 1: {e}", file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)

        # --- Step 2: Final Prompt + Context -> Final Response ---
        resposta_final_content = None
        if prompt_final_content:
            print(f"\nStep 2: Sending Final Prompt and Context to get the Final Response (Model: {GEMINI_MODEL})...")
            # Pass final prompt using keyword argument 'text='
            contents_etapa2 = [types.Part.from_text(text=prompt_final_content)] + context_parts
            try:
                 response_etapa2 = genai_client.models.generate_content(
                     model=GEMINI_MODEL,
                     contents=contents_etapa2 # Pass the list of parts directly
                 )
                 # Extract the final response
                 resposta_final_content = response_etapa2.text
                 print("  Final Response received:")
                 print("  ```")
                 print(resposta_final_content.strip())
                 print("  ```")
            except errors.APIError as e:
                print(f"  Gemini API Error (Step 2): Code {e.code} - {e.message}", file=sys.stderr)
                print(f"  Error details: {e}") # Print more details if available
                sys.exit(1)
            except Exception as e:
                print(f"  Unexpected Error in Step 2: {e}", file=sys.stderr)
                traceback.print_exc()
                sys.exit(1)
        else:
            print("Error: Could not get the final prompt from Step 1. Aborting Step 2.", file=sys.stderr)
            sys.exit(1)
        # --- End Two-Step Interaction ---

        # --- Save Final Response (AC8) ---
        if resposta_final_content:
            print("\nSaving Final Response...")
            save_llm_response(selected_task, resposta_final_content.strip())
        else:
             print("\nNo final response was generated to save.", file=sys.stderr)
        # --- End Save Final Response ---


    except SystemExit as e:
        # Capture sys.exit() called by argparse on error or -h
        # Avoids the generic "Unexpected error" message in these cases
        if e.code != 0: # If not a normal exit (like -h)
             print(f"Argument parsing error.", file=sys.stderr)
        sys.exit(e.code) # Propagate the original exit code
    except Exception as e:
        print(f"\nUnexpected error during execution: {e}", file=sys.stderr)
        traceback.print_exc() # Print the full traceback
        sys.exit(1)